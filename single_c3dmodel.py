# -*- coding: utf-8 -*-
"""single C3Dmodel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eSCXHDvOoYk7wx9e6TvsquBLBuhvb_rd
"""

from keras.models import Sequential,Model
from keras.layers.core import Dense, Dropout, Flatten
from keras.layers.convolutional import Convolution3D, MaxPooling3D, ZeroPadding3D
from tensorflow.keras.optimizers import SGD
from keras.layers import Add,Input,Conv3D,AveragePooling3D

def C3D_model(summary=False):
    """ Return the Keras model of the network
    """
    model = Sequential()
    model.add(AveragePooling3D(pool_size=(1, 2, 2), strides=(1,2,2)))
    # 1st layer group
    # input are 16 frames with size 112*112 with 3 channels
    model.add(Convolution3D(64, (3,3,3), activation='relu', 
                            padding='same', name='conv1',
                            strides=(1, 1, 1), 
                            input_shape=(16, 112, 112,3)))
    model.add(MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), 
                           padding='valid', name='pool1'))
    # 2nd layer group
    model.add(Convolution3D(128, (3,3,3), activation='relu', 
                            padding='same', name='conv2',
                            strides=(1, 1, 1)))
    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), 
                           padding='valid', name='pool2'))
    # 3rd layer group
    model.add(Convolution3D(256, (3,3,3), activation='relu', 
                            padding='same', name='conv3a',
                            strides=(1, 1, 1)))
    model.add(Convolution3D(256, (3,3,3), activation='relu', 
                            padding='same', name='conv3b',
                            strides=(1, 1, 1)))
    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), 
                           padding='valid', name='pool3'))
    # 4th layer group
    model.add(Convolution3D(512, (3,3,3), activation='relu', 
                            padding='same', name='conv4a',
                            strides=(1, 1, 1)))
    model.add(Convolution3D(512, (3,3,3), activation='relu', 
                            padding='same', name='conv4b',
                            strides=(1, 1, 1)))
    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), 
                           padding='valid', name='pool4'))
    # 5th layer group
    model.add(Convolution3D(512, (3,3,3), activation='relu', 
                            padding='same', name='conv5a',
                            strides=(1, 1, 1)))
    model.add(Convolution3D(512, (3,3,3), activation='relu', 
                            padding='same', name='conv5b',
                            strides=(1, 1, 1)))
    model.add(ZeroPadding3D(padding=(0, 1, 1)))
    model.add(MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), 
                           padding='valid', name='pool5'))
    model.add(Flatten())
    # FC layers group
    model.add(Dense(4096, activation='relu', name='fc6'))
    model.add(Dropout(.5))
    model.add(Dense(4096, activation='relu', name='fc7'))
    model.add(Dropout(.5))
    model.add(Dense(487, activation='softmax', name='fc8'))
    return model
model = C3D_model(True)
model.build(input_shape=(None,16,224,224,3))
model.summary()
hist = model.compile(optimizer="Adam", loss="sparse_categorical_crossentropy", metrics=["acc"])

import sys
from google.colab import drive
drive.mount('/content/drive')

import pickle
import numpy as np
import pandas as pd
import os, glob
import gc

def train_batch_generator(train_data_dir = '/content/drive/MyDrive/FER2021fall/data/'):
    files = sorted(glob.glob(os.path.join(train_data_dir,'*.pkl')))
    label_file_path = os.path.join(train_data_dir,'labels.csv')
    f = pd.read_csv(label_file_path)
    print('going to operate on:',files)
    vals, labels = [], []
    count = len(files)
    for file in files:
        print('Reading file {0}...........'.format(file))
        gc.collect()
        with open(file, 'rb') as rf:
            data = pickle.load(rf)
        vals.append(data.values())
        start = (len(files) - count) * 2000
        labels.append(f['label'][start:start+2000])
        count -= 1
        if count == 0:
            X_train = np.concatenate(vals)
            y_train = np.concatenate(labels)
            gc.collect()
            count = len(files)
            vals, labels = [], []
            yield(X_train, y_train)

for x_train, y_train in train_batch_generator():
    gc.collect()
    model.fit(x_train, y_train, batch_size = 16, epochs = 1)

#model.fit(new_val[:500],label[:500],epochs=10,batch_size=8)

results = model.evaluate(np.array(test_val), np.array(test_label))